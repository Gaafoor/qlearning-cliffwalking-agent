# CliffWalking with Tabular Q-Learning

This repository implements a reinforcement learning agent using **Tabular Q-Learning**
to navigate the `CliffWalking-v0` environment provided by OpenAI Gym.

The objective is to train an agent to reach a goal on a 4Ã—12 grid while avoiding a
dangerous cliff region that causes heavy penalties and resets.

Two variants of the learning strategy are implemented:

- **Greedy policy** (no explicit exploration)
- **Epsilon-greedy policy** with linear decay

The agent learns optimal behavior through repeated interaction with the environment,
gradually improving its decision-making using Q-values.

---

## ğŸ—º Environment Overview

The CliffWalking environment is represented as a 4Ã—12 grid:

![CliffWalking Map](IMG_9942.jpeg)


- `S` â†’ Start position  
- `G` â†’ Goal position  
- `X` â†’ Cliff cells (large negative reward + reset)

At each step:
- Moving yields **-1 reward**
- Falling off the cliff yields **-100 reward**

---

## ğŸ¯ Learning Objective

The agent must:
- Reach the goal efficiently
- Avoid the cliff
- Minimize total negative reward

â¸»

ğŸš€ Running the Code

Greedy version

python q_learning_greedy.py

Epsilon-greedy version

python q_learning_epsilon.py

Plots will appear automatically showing rewards per episode.

â¸»

ğŸ” Q-Learning Update Rule

The agent updates its Q-values based on the Bellman optimality equation:

[
Q(s,a) \leftarrow Q(s,a) + \alpha [r + \gamma \max_{aâ€™} Q(sâ€™,aâ€™) - Q(s,a)]
]

Where:
	â€¢	Î± = learning rate
	â€¢	Î³ = discount factor
	â€¢	r = reward
	â€¢	sâ€² = next state

â¸»

âš ï¸ Exploration vs Exploitation

Pure greedy policies can become stuck in suboptimal behavior.

To avoid this, the epsilon-greedy method:
	â€¢	Uses random actions early (exploration)
	â€¢	Gradually reduces randomness (decay)
	â€¢	Exploits knowledge later

â¸»

ğŸ“ˆ Training Results

Both implementations produce a plot of episode return across training.

Typical behavior observed:
	â€¢	Greedy strategy learns slowly and often takes safe but longer paths
	â€¢	Epsilon-greedy converges to a shorter, more optimal route

Example learning curve:

(optional image if you export plots)

â¸»

ğŸ“Š Evaluation

After training, the agent is tested without exploration:
	â€¢	Always selects the highest-valued action
	â€¢	Runs for multiple episodes
	â€¢	Success rate and average returns are printed

â¸»

ğŸ§° Hyperparameters

Parameter	Value
Episodes	1000
Learning rate Î±	0.5
Discount Î³	0.9
Îµ initial	1.0
Îµ decay	0.001

Feel free to experiment with them.

â¸»

ğŸ§ª Experiment Ideas

You can expand this project by adding:
	â€¢	SARSA comparison
	â€¢	Different Îµ decay strategies
	â€¢	Randomized start positions
	â€¢	Visualization of agent trajectories
	â€¢	Heatmaps of visited states

â¸»

ğŸ§© Key Concepts Demonstrated
	â€¢	Reinforcement learning fundamentals
	â€¢	Tabular value updates
	â€¢	Exploration strategies
	â€¢	Deterministic vs risky policy trade-offs
	â€¢	Convergence behavior

â¸»

ğŸ”§ Compatibility
	â€¢	Gym (v0.26+) new step API supported
	â€¢	Python 3.8+

â¸»

Author
â€¢	Mostafa Ahmed
